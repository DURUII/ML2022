@article{f22b4182cdbe1b564f237ab6835376125248e9a6,
title = {Generalized End-to-End Loss for Speaker Verification},
year = {2017},
url = {https://www.semanticscholar.org/paper/f22b4182cdbe1b564f237ab6835376125248e9a6},
abstract = {In this paper, we propose a new loss function called generalized end-to-end (GE2E) loss, which makes the training of speaker verification models more efficient than our previous tuple-based end-to-end (TE2E) loss function. Unlike TE2E, the GE2E loss function updates the network in a way that emphasizes examples that are difficult to verify at each step of the training process. Additionally, the GE2E loss does not require an initial stage of example selection. With these properties, our model with the new loss function decreases speaker verification EER by more than 10%, while reducing the training time by 60% at the same time. We also introduce the MultiReader technique, which allows us to do domain adaptation - training a more accurate model that supports multiple keywords (i.e., “OK Google” and “Hey Google”) as well as multiple dialects.},
author = {Li Wan and Quan Wang and Alan Papir and I. Lopez-Moreno},
doi = {10.1109/ICASSP.2018.8462665},
arxivid = {1710.10467},
}

@article{6e08198a32c8cbe6e34205d56cc4e9d2464d5f62,
title = {Developing On-Line Speaker Diarization System},
year = {2017},
url = {https://www.semanticscholar.org/paper/6e08198a32c8cbe6e34205d56cc4e9d2464d5f62},
abstract = {In this paper we describe the process of converting a research prototype system for Speaker Diarization into a fully deployed product running in real time and with low latency. The deployment is a part of the IBM Cloud Speech-to-Text (STT) Service. First, the prototype system is described and the requirements for the on-line, deployable system are introduced. Then we describe the technical approaches we took to satisfy these requirements and discuss some of the challenges we have faced. In particular, we present novel ideas for speeding up the system by using Automatic Speech Recognition (ASR) transcripts as an input to diarization, we introduce a concept of active window to keep the computational complexity linear, we improve the speaker model using a new speaker-clustering algorithm, we automatically keep track of the number of active speakers and we enable the users to set an operating point on a continuous scale between low latency and optimal accuracy. The deployed system has been tuned on real-life data reaching average Speaker Error Rates around 3% and improving over the prototype system by about 10% relative.},
author = {D. Dimitriadis and P. Fousek},
doi = {10.21437/INTERSPEECH.2017-166},
}

@article{4a64008cd21488071ac418aad32ce7129408edb4,
title = {Attentive Statistics Pooling for Deep Speaker Embedding},
year = {2018},
url = {https://www.semanticscholar.org/paper/4a64008cd21488071ac418aad32ce7129408edb4},
abstract = {This paper proposes attentive statistics pooling for deep speaker embedding in text-independent speaker verification. In conventional speaker embedding, frame-level features are averaged over all the frames of a single utterance to form an utterance-level feature. Our method utilizes an attention mechanism to give different weights to different frames and generates not only weighted means but also weighted standard deviations. In this way, it can capture long-term variations in speaker characteristics more effectively. An evaluation on the NIST SRE 2012 and the VoxCeleb data sets shows that it reduces equal error rates (EERs) from the conventional method by 7.5% and 8.1%, respectively.},
author = {K. Okabe and Takafumi Koshinaka and K. Shinoda},
doi = {10.21437/Interspeech.2018-993},
arxivid = {1803.10963},
}

@article{f59ccf1b1385071c19c4b9e05d041af828490a56,
title = {Lightweight Embeddings for Speaker Verification},
year = {2018},
url = {https://www.semanticscholar.org/paper/f59ccf1b1385071c19c4b9e05d041af828490a56},
abstract = {S2 TL;DR: This paper presents speaker verification (SV) system using deep neural networks with hash representations (binarization) of embeddings, which decreases the embedding memory size in 32x times and increases the system evaluation performance.},
author = {M. Tkachenko and Alexander Yamshinin and M. Kotov and Marina Nastasenko},
doi = {10.1007/978-3-319-99579-3_70},
}

@article{d9dbdd254b02ef1af2769af403cba373c1b1bcb1,
title = {End-to-End Neural Speaker Diarization with Permutation-Free Objectives},
year = {2019},
url = {https://www.semanticscholar.org/paper/d9dbdd254b02ef1af2769af403cba373c1b1bcb1},
abstract = {In this paper, we propose a novel end-to-end neural-network-based speaker diarization method. Unlike most existing methods, our proposed method does not have separate modules for extraction and clustering of speaker representations. Instead, our model has a single neural network that directly outputs speaker diarization results. To realize such a model, we formulate the speaker diarization problem as a multi-label classification problem, and introduces a permutation-free objective function to directly minimize diarization errors without being suffered from the speaker-label permutation problem. Besides its end-to-end simplicity, the proposed method also benefits from being able to explicitly handle overlapping speech during training and inference. Because of the benefit, our model can be easily trained/adapted with real-recorded multi-speaker conversations just by feeding the corresponding multi-speaker segment labels. We evaluated the proposed method on simulated speech mixtures. The proposed method achieved diarization error rate of 12.28%, while a conventional clustering-based system produced diarization error rate of 28.77%. Furthermore, the domain adaptation with real-recorded speech provided 25.6% relative improvement on the CALLHOME dataset. Our source code is available online at this https URL.},
author = {Yusuke Fujita and Naoyuki Kanda and Shota Horiguchi and Kenji Nagamatsu and Shinji Watanabe},
doi = {10.21437/interspeech.2019-2899},
arxivid = {1909.05952},
}

@article{25905976d285338fd7de83a097d158c9bac5dd5a,
title = {End-to-End Trainable Self-Attentive Shallow Network for Text-Independent Speaker Verification},
year = {2020},
url = {https://www.semanticscholar.org/paper/25905976d285338fd7de83a097d158c9bac5dd5a},
abstract = {Generalized end-to-end (GE2E) model is widely used in speaker verification (SV) fields due to its expandability and generality regardless of specific languages. However, the long-short term memory (LSTM) based on GE2E has two limitations: First, the embedding of GE2E suffers from vanishing gradient, which leads to performance degradation for very long input sequences. Secondly, utterances are not represented as a properly fixed dimensional vector. In this paper, to overcome issues mentioned above, we propose a novel framework for SV, end-to-end trainable self-attentive shallow network (SASN), incorporating a time-delay neural network (TDNN) and a self-attentive pooling mechanism based on the self-attentive x-vector system during an utterance embedding phase. We demonstrate that the proposed model is highly efficient, and provides more accurate speaker verification than GE2E. For VCTK dataset, with just less than half the size of GE2E, the proposed model showed significant performance improvement over GE2E of about 63%, 67%, and 85% in EER (Equal error rate), DCF (Detection cost function), and AUC (Area under the curve), respectively. Notably, when the input length becomes longer, the DCF score improvement of the proposed model is about 17 times greater than that of GE2E.},
author = {Hyeonmook Park and Jungbae Park and Sang Wan Lee},
arxivid = {2008.06146},
}

@article{c340b89e7b7fa84fac85cdcf38ba7007e2e71930,
title = {End-to-End Neural Speaker Diarization with Self-Attention},
year = {2019},
url = {https://www.semanticscholar.org/paper/c340b89e7b7fa84fac85cdcf38ba7007e2e71930},
abstract = {Speaker diarization has been mainly developed based on the clustering of speaker embeddings. However, the clustering-based approach has two major problems; i.e., (i) it is not optimized to minimize diarization errors directly, and (ii) it cannot handle speaker overlaps correctly. To solve these problems, the End-to-End Neural Diarization (EEND), in which a bidirectional long short-term memory (BLSTM) network directly outputs speaker diarization results given a multi-talker recording, was recently proposed. In this study, we enhance EEND by introducing self-attention blocks instead of BLSTM blocks. In contrast to BLSTM, which is conditioned only on its previous and next hidden states, self-attention is directly conditioned on all the other frames, making it much suitable for dealing with the speaker diarization problem. We evaluated our proposed method on simulated mixtures, real telephone calls, and real dialogue recordings. The experimental results revealed that the self-attention was the key to achieving good performance and that our proposed method performed significantly better than the conventional BLSTM-based method. Our method was even better than that of the state-of-the-art x-vector clustering-based method. Finally, by visualizing the latent representation, we show that the self-attention can capture global speaker characteristics in addition to local speech activity dynamics. Our source code is available online at https://github.com/hitachi-speech/EEND.},
author = {Yusuke Fujita and Naoyuki Kanda and Shota Horiguchi and Yawen Xue and Kenji Nagamatsu and Shinji Watanabe},
doi = {10.1109/ASRU46091.2019.9003959},
arxivid = {1909.06247},
}

@article{ea5408ee3c232023baf8e2e9e3f75e6c32761eff,
title = {A Cascade Architecture for Keyword Spotting on Mobile Devices},
year = {2017},
url = {https://www.semanticscholar.org/paper/ea5408ee3c232023baf8e2e9e3f75e6c32761eff},
abstract = {We present a cascade architecture for keyword spotting with speaker verification on mobile devices. By pairing a small computational footprint with specialized digital signal processing (DSP) chips, we are able to achieve low power consumption while continuously listening for a keyword.},
author = {Alexander Gruenstein and R. Álvarez and Chris Thornton and M. Ghodrat},
arxivid = {1712.03603},
}

@article{f8c61521ac186443aae4082616821a55780d32a9,
title = {Deep Speaker: an End-to-End Neural Speaker Embedding System},
year = {2017},
url = {https://www.semanticscholar.org/paper/f8c61521ac186443aae4082616821a55780d32a9},
abstract = {We present Deep Speaker, a neural speaker embedding system that maps utterances to a hypersphere where speaker similarity is measured by cosine similarity. The embeddings generated by Deep Speaker can be used for many tasks, including speaker identification, verification, and clustering. We experiment with ResCNN and GRU architectures to extract the acoustic features, then mean pool to produce utterance-level speaker embeddings, and train using triplet loss based on cosine similarity. Experiments on three distinct datasets suggest that Deep Speaker outperforms a DNN-based i-vector baseline. For example, Deep Speaker reduces the verification equal error rate by 50% (relatively) and improves the identification accuracy by 60% (relatively) on a text-independent dataset. We also present results that suggest adapting from a model trained with Mandarin can improve accuracy for English speaker recognition.},
author = {Chao Li and Xiaokong Ma and B. Jiang and Xiangang Li and Xuewei Zhang and Xiao Liu and Ying Cao and Ajay Kannan and Zhenyao Zhu},
arxivid = {1705.02304},
}

@article{abd5484852e7c0d62ee776cbd0b8d8ff3b26bb24,
title = {Attention-Based Models for Text-Dependent Speaker Verification},
year = {2017},
url = {https://www.semanticscholar.org/paper/abd5484852e7c0d62ee776cbd0b8d8ff3b26bb24},
abstract = {Attention-based models have recently shown great performance on a range of tasks, such as speech recognition, machine translation, and image captioning due to their ability to summarize relevant information that expands through the entire length of an input sequence. In this paper, we analyze the usage of attention mechanisms to the problem of sequence summarization in our end-to-end text-dependent speaker recognition system. We explore different topologies and their variants of the attention layer, and compare different pooling methods on the attention weights. Ultimately, we show that attention-based models can improves the Equal Error Rate (EER) of our speaker verification system by relatively 14% compared to our non-attention LSTM baseline model.},
author = {F. R. Chowdhury and Quan Wang and I. Lopez-Moreno and Li Wan},
doi = {10.1109/ICASSP.2018.8461587},
arxivid = {1710.10470},
}

@article{c45dd304657821c89cf87e4ec1eddb0f238c6dd1,
title = {Voxceleb: Large-scale speaker veri ﬁ cation in the wild I},
year = {2019},
url = {https://www.semanticscholar.org/paper/c45dd304657821c89cf87e4ec1eddb0f238c6dd1},
abstract = {The objective of this workisspeaker recognition undernoisy and unconstrained conditions. We maketwo key contributions. First, we introduce a very large-scale audio-visual dataset collected from open source media using a fully automated pipeline . Most existing datasets for speaker identi ﬁ cation contain samples obtainedunderquite constrained conditions, and usuallyrequire manual annotations, hence are limited in size. We propose a pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker veri ﬁ cation using a two-stream synchronization Convolutional Neural Network (CNN), and con ﬁ rming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains contains over a million ‘ real-world ’ utterances from over 6000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare different CNN architectures with various aggregation methods and training loss functions that can effectively recognise identities from voice under various conditions. The models trained on our dataset surpass the performance of previous works by a signi ﬁ cant margin.},
author = {Arsha Nagrani and Joon Son Chung and Weidi Xie and Andrew Zissermana},
}

@article{d7de3f4d5bd36df1c3d05d7c9da8da3b923b2ced,
title = {In defence of metric learning for speaker recognition},
year = {2020},
url = {https://www.semanticscholar.org/paper/d7de3f4d5bd36df1c3d05d7c9da8da3b923b2ced},
abstract = {The objective of this paper is 'open-set' speaker recognition of unseen speakers, where ideal embeddings should be able to condense information into a compact utterance-level representation that has small intra-speaker and large inter-speaker distance. 
A popular belief in speaker recognition is that networks trained with classification objectives outperform metric learning methods. In this paper, we present an extensive evaluation of most popular loss functions for speaker recognition on the VoxCeleb dataset. We demonstrate that the vanilla triplet loss shows competitive performance compared to classification-based losses, and those trained with our proposed metric learning objective outperform state-of-the-art methods.},
author = {Joon Son Chung and Jaesung Huh and Seongkyu Mun and Minjae Lee and Hee-Soo Heo and Soyeon Choe and Chiheon Ham and Sung-Ye Jung and Bong-Jin Lee and Icksang Han},
doi = {10.21437/interspeech.2020-1064},
arxivid = {2003.11982},
}

@article{cf6352c789ab51320fa7ca9b1440c685b57fd769,
title = {Diarization is Hard: Some Experiences and Lessons Learned for the JHU Team in the Inaugural DIHARD Challenge},
year = {2018},
url = {https://www.semanticscholar.org/paper/cf6352c789ab51320fa7ca9b1440c685b57fd769},
abstract = {We describe in this paper the experiences of the Johns Hopkins University team during the inaugural DIHARD diarization evaluation. This new task provided microphone recordings in a variety of difficult conditions and challenged researchers to fully consider all speaker activity, without the currently typical practices of unscored collars or ignored overlapping speaker segments. This paper explores several key aspects of currently state-of-the-art diarization methods, such as training data selection, signal bandwidth for feature extraction, representations of speech segments (i-vector versus x-vector), and domainadaptive processing. In the end, our best system clustered xvector embeddings trained on wideband microphone data followed by Variational-Bayesian refinement, and a speech activity detector specifically trained for this task with in-domain data was found to be the best performing. After presenting these decisions and their final result, we discuss lessons learned and remaining challenges within the lens of this new approach to diarization performance measurement.},
author = {Gregory Sell and David Snyder and A. McCree and D. Garcia-Romero and J. Villalba and Matthew Maciejewski and Vimal Manohar and N. Dehak and Daniel Povey and Shinji Watanabe and S. Khudanpur},
doi = {10.21437/Interspeech.2018-1893},
}

@article{f94a09614d0b6f5c7ac6ab4b0f6df21480f0c7a5,
title = {Voxceleb: Large-scale speaker verification in the wild},
year = {2020},
url = {https://www.semanticscholar.org/paper/f94a09614d0b6f5c7ac6ab4b0f6df21480f0c7a5},
abstract = {S2 TL;DR: A very large-scale audio-visual dataset collected from open source media using a fully automated pipeline and developed and compared different CNN architectures with various aggregation methods and training loss functions that can effectively recognise identities from voice under various conditions are introduced.},
author = {Arsha Nagrani and Joon Son Chung and Weidi Xie and Andrew Zisserman},
doi = {10.1016/j.csl.2019.101027},
}

@article{da45697cbbf1c4891f37fc679a274a6ec4d2dcc0,
title = {Speaker Recognition for Multi-speaker Conversations Using X-vectors},
year = {2019},
url = {https://www.semanticscholar.org/paper/da45697cbbf1c4891f37fc679a274a6ec4d2dcc0},
abstract = {Recently, deep neural networks that map utterances to fixed-dimensional embeddings have emerged as the state-of-the-art in speaker recognition. Our prior work introduced x-vectors, an embedding that is very effective for both speaker recognition and diarization. This paper combines our previous work and applies it to the problem of speaker recognition on multi-speaker conversations. We measure performance on Speakers in the Wild and report what we believe are the best published error rates on this dataset. Moreover, we find that diarization substantially reduces error rate when there are multiple speakers, while maintaining excellent performance on single-speaker recordings. Finally, we introduce an easily implemented method to remove the domain-sensitive threshold typically used in the clustering stage of a diarization system. The proposed method is more robust to domain shifts, and achieves similar results to those obtained using a well-tuned threshold.},
author = {David Snyder and D. Garcia-Romero and Gregory Sell and A. McCree and Daniel Povey and S. Khudanpur},
doi = {10.1109/ICASSP.2019.8683760},
}

@article{ef8cebdb9e05c4f12e5019531897ad5f2e29afaa,
title = {End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances},
year = {2017},
url = {https://www.semanticscholar.org/paper/ef8cebdb9e05c4f12e5019531897ad5f2e29afaa},
abstract = {Text-independent speaker verification against short utterances is still challenging despite of recent advances in the field of speaker recognition with i-vector framework. In general, to get a robust i-vector representation, a satisfying amount of data is needed in the MAP adaptation step, which is hard to meet under short duration constraint. To overcome this, we present an endto-end system which directly learns a mapping from speech features to a compact fixed length speaker discriminative embedding where the Euclidean distance is employed for measuring similarity within trials. To learn the feature mapping, a modified Inception Net with residual block is proposed to optimize the triplet loss function. The input of our end-to-end system is a fixed length spectrogram converted from an arbitrary length utterance. Experiments show that our system consistently outperforms a conventional i-vector system on short duration speaker verification tasks. To test the limit under various duration conditions, we also demonstrate how our end-to-end system behaves with different duration from 2s-4s.},
author = {Chunlei Zhang and K. Koishida},
doi = {10.21437/INTERSPEECH.2017-1608},
}

@article{27b18a6ce0971d17534391c2b394e1ad4d01e703,
title = {An Empirical Study on Text-Independent Speaker Verification based on the GE2E Method},
year = {2020},
url = {https://www.semanticscholar.org/paper/27b18a6ce0971d17534391c2b394e1ad4d01e703},
abstract = {While many researchers in the speaker recognition area have started to replace the former classical state-of-the-art methods with deep learning techniques, some of the traditional i-vector-based methods are still state-of-the-art in the context of text-independent speaker verification. Google’s Generalized Endto-End Loss for Speaker Verification (GE2E), a deep learningbased technique using long short-term memory units, has recently gained a lot of attention due to its speed in convergence and generalization. In this study, we aim at further studying the GE2E method and comparing different scenarios in order to investigate all of its aspects. Various experiments including the effects of a random sampling of test and enrollment utterances, test utterance duration, and the number of enrollment utterances are discussed in this article. Furthermore, we compare the GE2E method with the baseline state-of-the-art i-vector-based methods for textindependent speaker verification and show that it outperforms them by resulting in lower error rates while being end-to-end and requiring less training time for convergence.},
author = {Soroosh Tayebi Arasteh},
arxivid = {2011.04896},
}

@article{16d55d64f99997954a72011cb4233fe5a8e6fded,
title = {Syllable-Dependent Discriminative Learning for Small Footprint Text-Dependent Speaker Verification},
year = {2019},
url = {https://www.semanticscholar.org/paper/16d55d64f99997954a72011cb4233fe5a8e6fded},
abstract = {This study proposes a novel scheme of syllable-dependent discriminative speaker embedding learning for small footprint text-dependent speaker verification systems. To suppress undesired syllable variation and enhance the power of discrimination inherited in the frame-level features, we design a novel syllable-dependent clustering loss to optimize the network. Specifically, this loss function utilizes syllable labels as auxiliary supervision information to explicitly maximize inter-syllable divisibility and intra-syllable compactness between the learned frame-level features. Successively, we propose two syllable-dependent pooling mechanisms to aggregate the frame-level features to several syllable-level features by averaging those features corresponding to each syllable. The utterance-level speaker embeddings with powerful discrimination are then obtained by concatenating the syllable-level features. Experimental results on Tencent voice wake-up dataset show that our proposed scheme can accelerate the network convergence and achieve significant performance improvement against the state-of-the-art methods.},
author = {Junyi Peng and Yuexian Zou and N. Li and Deyi Tuo and Dan Su and Meng Yu and Chunlei Zhang and Dong Yu},
doi = {10.1109/ASRU46091.2019.9004023},
}

@article{8a26431833b0ea8659ef1d24bff3ac9e56dcfcd0,
title = {VoxCeleb: A Large-Scale Speaker Identification Dataset},
year = {2017},
url = {https://www.semanticscholar.org/paper/8a26431833b0ea8659ef1d24bff3ac9e56dcfcd0},
abstract = {Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and are usually hand-annotated, hence limited in size. The goal of this paper is to generate a large scale text-independent speaker identification dataset collected 'in the wild'. We make two contributions. First, we propose a fully automated pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains hundreds of thousands of 'real world' utterances for over 1,000 celebrities. Our second contribution is to apply and compare various state of the art speaker identification techniques on our dataset to establish baseline performance. We show that a CNN based architecture obtains the best performance for both identification and verification.},
author = {Arsha Nagrani and Joon Son Chung and Andrew Zisserman},
doi = {10.21437/INTERSPEECH.2017-950},
arxivid = {1706.08612},
}

@article{7061c4c9473856a348c6e3f025b908145c50a674,
title = {Robust End-to-End Speaker Verification Using EEG},
year = {2020},
url = {},
abstract = {S2 TL;DR: It is demonstrated that performance of a speaker verification system can be improved by concatenating electroencephalography (EEG) signal features with speech signal features or only using EEG signal features.},
author = {Yan Han},
}

@article{a327c57ef2f32ed8e55580a7d1b6b52c310c2e04,
title = {Max Margin Cosine Loss for Speaker Identification on Short Utterances},
year = {2018},
url = {https://www.semanticscholar.org/paper/a327c57ef2f32ed8e55580a7d1b6b52c310c2e04},
abstract = {Speaker identification has made extraordinary progress owing to the advancement of deep neural networks. Speaker feature discrimination is a vital term in speaker recognition. However, the traditional softmax loss usually lacks the power of discrimination. To address this problem, this paper explores a novel loss function, namely max margin cosine loss (MMCL). To be specific, we realize the function by L2 normalizing both features and weight vectors in the softmax loss, together with a cosine margin term to maximize the decision margin in the angular space. In addition, max margin constraint, as one regularization term, is incorporated into the proposed loss function. Experimental results demonstrate the effectiveness of our proposed max margin cosine loss and superiority over pervious losses. For example, on 2s condition, MMCL reduces the equal error rate by 10.63% relatively compared to additive angular margin cosine loss (AMCL), while AMCL has already obtained 6.37% relative reduction than softmax loss.1},
author = {Ruifang Ji and Junhua Cao and Xinyuan Cai and Bo Xu},
doi = {10.1109/ISCSLP.2018.8706654},
}

@article{afb5e74474e239b32b97aa27e13ae2f83c171de8,
title = {Self-Attentive Speaker Embeddings for Text-Independent Speaker Verification},
year = {2018},
url = {https://www.semanticscholar.org/paper/afb5e74474e239b32b97aa27e13ae2f83c171de8},
abstract = {This paper introduces a new method to extract speaker embeddings from a deep neural network (DNN) for text-independent speaker veriﬁcation. Usually, speaker embeddings are extracted from a speaker-classiﬁcation DNN that averages the hidden vectors over the frames of a speaker; the hidden vectors produced from all the frames are assumed to be equally important. We relax this assumption and compute the speaker embedding as a weighted average of a speaker’s frame-level hidden vectors, and their weights are automatically determined by a self-attention mechanism. The effect of multiple attention heads are also investigated to capture different aspects of a speaker’s input speech. Finally, a PLDA classiﬁer is used to compare pairs of embeddings. The proposed self-attentive speaker embedding system is compared with a strong DNN embedding baseline on NIST SRE 2016. We ﬁnd that the self-attentive embeddings achieve superior performance. Moreover, the improvement produced by the self-attentive speaker embeddings is consistent with both short and long testing utterances.},
author = {Yingke Zhu and Tom Ko and David Snyder and B. Mak and Daniel Povey},
doi = {10.21437/Interspeech.2018-1158},
}

@article{a917a31e4e32e81c49096702cfe05c899c313e7a,
title = {A novel scheme for speaker recognition using a phonetically-aware deep neural network},
year = {2014},
url = {https://www.semanticscholar.org/paper/a917a31e4e32e81c49096702cfe05c899c313e7a},
abstract = {We propose a novel framework for speaker recognition in which extraction of sufficient statistics for the state-of-the-art i-vector model is driven by a deep neural network (DNN) trained for automatic speech recognition (ASR). Specifically, the DNN replaces the standard Gaussian mixture model (GMM) to produce frame alignments. The use of an ASR-DNN system in the speaker recognition pipeline is attractive as it integrates the information from speech content directly into the statistics, allowing the standard backends to remain unchanged. Improvement from the proposed framework compared to a state-of-the-art system are of 30% relative at the equal error rate when evaluated on the telephone conditions from the 2012 NIST speaker recognition evaluation (SRE). The proposed framework is a successful way to efficiently leverage transcribed data for speaker recognition, thus opening up a wide spectrum of research directions.},
author = {Yun Lei and N. Scheffer and L. Ferrer and Mitchell McLaren},
doi = {10.1109/ICASSP.2014.6853887},
}

@article{40e8d23231469e6495d3e06086e64df93e9dcfa0,
title = {Front-End Factor Analysis for Speaker Verification},
year = {2011},
url = {https://www.semanticscholar.org/paper/40e8d23231469e6495d3e06086e64df93e9dcfa0},
abstract = {This paper presents an extension of our previous work which proposes a new speaker representation for speaker verification. In this modeling, a new low-dimensional speaker- and channel-dependent space is defined using a simple factor analysis. This space is named the total variability space because it models both speaker and channel variabilities. Two speaker verification systems are proposed which use this new representation. The first system is a support vector machine-based system that uses the cosine kernel to estimate the similarity between the input data. The second system directly uses the cosine similarity as the final decision score. We tested three channel compensation techniques in the total variability space, which are within-class covariance normalization (WCCN), linear discriminate analysis (LDA), and nuisance attribute projection (NAP). We found that the best results are obtained when LDA is followed by WCCN. We achieved an equal error rate (EER) of 1.12% and MinDCF of 0.0094 using the cosine distance scoring on the male English trials of the core condition of the NIST 2008 Speaker Recognition Evaluation dataset. We also obtained 4% absolute EER improvement for both-gender trials on the 10 s-10 s condition compared to the classical joint factor analysis scoring.},
author = {N. Dehak and P. Kenny and Réda Dehak and P. Dumouchel and P. Ouellet},
doi = {10.1109/TASL.2010.2064307},
}

@article{507271f26a1c40bb128d8e09289e2054d304e49c,
title = {Speaker Diarization with LSTM},
year = {2017},
url = {https://www.semanticscholar.org/paper/507271f26a1c40bb128d8e09289e2054d304e49c},
abstract = {For many years, i-vector based audio embedding techniques were the dominant approach for speaker verification and speaker diarization applications. However, mirroring the rise of deep learning in various domains, neural network based audio embeddings, also known as d-vectors, have consistently demonstrated superior speaker verification performance. In this paper, we build on the success of d-vector based speaker verification systems to develop a new d-vector based approach to speaker diarization. Specifically, we combine LSTM-based d-vector audio embeddings with recent work in nonparametric clustering to obtain a state-of-the-art speaker diarization system. Our system is evaluated on three standard public datasets, suggesting that d-vector based diarization systems offer significant advantages over traditional i-vector based systems. We achieved a 12.0% diarization error rate on NIST SRE 2000 CALLHOME, while our model is trained with out-of-domain data from voice search logs.},
author = {Quan Wang and Carlton Downey and Li Wan and P. A. Mansfield and I. Lopez-Moreno},
doi = {10.1109/ICASSP.2018.8462628},
arxivid = {1710.10468},
}

@article{e5dd20eb15f0e4811156cce8b4cf930bd002d43e,
title = {Speaker diarization using deep neural network embeddings},
year = {2017},
url = {https://www.semanticscholar.org/paper/e5dd20eb15f0e4811156cce8b4cf930bd002d43e},
abstract = {Speaker diarization is an important front-end for many speech technologies in the presence of multiple speakers, but current methods that employ i-vector clustering for short segments of speech are potentially too cumbersome and costly for the front-end role. In this work, we propose an alternative approach for learning representations via deep neural networks to remove the i-vector extraction process from the pipeline entirely. The proposed architecture simultaneously learns a fixed-dimensional embedding for acoustic segments of variable length and a scoring function for measuring the likelihood that the segments originated from the same or different speakers. Through tests on the CALLHOME conversational telephone speech corpus, we demonstrate that, in addition to streamlining the diarization architecture, the proposed system matches or exceeds the performance of state-of-the-art baselines. We also show that, though this approach does not respond as well to unsupervised calibration strategies as previous systems, the incorporation of well-founded speaker priors sufficiently mitigates this shortcoming.},
author = {D. Garcia-Romero and David Snyder and Gregory Sell and Daniel Povey and A. McCree},
doi = {10.1109/ICASSP.2017.7953094},
}

@article{f500a4da21f47725b0a180c41b56693a5db66252,
title = {Utterance-level Aggregation for Speaker Recognition in the Wild},
year = {2019},
url = {https://www.semanticscholar.org/paper/f500a4da21f47725b0a180c41b56693a5db66252},
abstract = {The objective of this paper is speaker recognition ‘in the wild’ – where utterances may be of variable length and also contain irrelevant signals. Crucial elements in the design of deep networks for this task are the type of trunk (frame level) network, and the method of temporal aggregation. We propose a powerful speaker recognition deep network, using a ‘thin-ResNet’ trunk architecture, and a dictionary-based NetVLAD or GhostVLAD layer to aggregate features across time, that can be trained end-to-end. We show that our network achieves state of the art performance by a significant margin on the VoxCeleb1 test set for speaker recognition, whilst requiring fewer parameters than previous methods. We also investigate the effect of utterance length on performance, and conclude that for ‘in the wild’ data, a longer length is beneficial.},
author = {Weidi Xie and Arsha Nagrani and Joon Son Chung and Andrew Zisserman},
doi = {10.1109/ICASSP.2019.8683120},
arxivid = {1902.10107},
}

@article{3ef8ec08dbce0675e14dd0750e0ba63b94470194,
title = {Generalized LSTM-based End-to-End Text-Independent Speaker Verification},
year = {2020},
url = {https://www.semanticscholar.org/paper/3ef8ec08dbce0675e14dd0750e0ba63b94470194},
abstract = {The increasing amount of available data and more affordable hardware solutions have opened a gate to the realm of Deep Learning (DL). Due to the rapid advancements and ever-growing popularity of DL, it has begun to invade almost every field, where machine learning is applicable, by altering the traditional state-of-the-art methods. While many researchers in the speaker recognition area have also started to replace the former state-of-the-art methods with DL techniques, some of the traditional i-vector-based methods are still state-of-the-art in the context of text-independent speaker verification (TI-SV). In this paper, we discuss the most recent generalized end-to-end (GE2E) DL technique based on Long Short-term Memory (LSTM) units for TI-SV by Google and compare different scenarios and aspects including utterance duration, training time, and accuracy to prove that our method outperforms the traditional methods.},
author = {Soroosh Tayebi Arasteh},
}

@article{312815d3897fce372a27f53041723ec61dd8c818,
title = {Speaker diarization with plda i-vector scoring and unsupervised calibration},
year = {2014},
url = {https://www.semanticscholar.org/paper/312815d3897fce372a27f53041723ec61dd8c818},
abstract = {Speaker diarization via unsupervised i-vector clustering has gained popularity in recent years. In this approach, i-vectors are extracted from short clips of speech segmented from a larger multi-speaker conversation and organized into speaker clusters, typically according to their cosine score. In this paper, we propose a system that incorporates probabilistic linear discriminant analysis (PLDA) for i-vector scoring, a method already frequently utilized in speaker recognition tasks, and uses unsupervised calibration of the PLDA scores to determine the clustering stopping criterion. We also demonstrate that denser sampling in the i-vector space with overlapping temporal segments provides a gain in the diarization task. We test our system on the CALLHOME conversational telephone speech corpus, which includes multiple languages and a varying number of speakers, and we show that PLDA scoring outperforms the same system with cosine scoring, and that overlapping segments reduce diarization error rate (DER) as well.},
author = {Gregory Sell and D. Garcia-Romero},
doi = {10.1109/SLT.2014.7078610},
}

@article{8875ae233bc074f5cd6c4ebba447b536a7e847a5,
title = {VoxCeleb2: Deep Speaker Recognition},
year = {2018},
url = {https://www.semanticscholar.org/paper/8875ae233bc074f5cd6c4ebba447b536a7e847a5},
abstract = {The objective of this paper is speaker recognition under noisy and unconstrained conditions. 
We make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset. 
Second, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin.},
author = {Joon Son Chung and Arsha Nagrani and Andrew Zisserman},
doi = {10.21437/Interspeech.2018-1929},
arxivid = {1806.05622},
}

@article{576e7eb1131c925179d0dd57f8c25b8adfc2e7bd,
title = {Deep neural networks for small footprint text-dependent speaker verification},
year = {2014},
url = {https://www.semanticscholar.org/paper/576e7eb1131c925179d0dd57f8c25b8adfc2e7bd},
abstract = {In this paper we investigate the use of deep neural networks (DNNs) for a small footprint text-dependent speaker verification task. At development stage, a DNN is trained to classify speakers at the frame-level. During speaker enrollment, the trained DNN is used to extract speaker specific features from the last hidden layer. The average of these speaker features, or d-vector, is taken as the speaker model. At evaluation stage, a d-vector is extracted for each utterance and compared to the enrolled speaker model to make a verification decision. Experimental results show the DNN based speaker verification system achieves good performance compared to a popular i-vector system on a small footprint text-dependent speaker verification task. In addition, the DNN based system is more robust to additive noise and outperforms the i-vector system at low False Rejection operating points. Finally the combined system outperforms the i-vector system by 14% and 25% relative in equal error rate (EER) for clean and noisy conditions respectively.},
author = {Ehsan Variani and X. Lei and E. McDermott and I. Lopez-Moreno and J. Gonzalez-Dominguez},
doi = {10.1109/ICASSP.2014.6854363},
}

@article{9bfe320025a45fc07e560c07ca8edc91e538d201,
title = {End-to-End attention based text-dependent speaker verification},
year = {2016},
url = {https://www.semanticscholar.org/paper/9bfe320025a45fc07e560c07ca8edc91e538d201},
abstract = {A new type of End-to-End system for text-dependent speaker verification is presented in this paper. Previously, using the phonetic discriminate/speaker discriminate DNN as a feature extractor for speaker verification has shown promising results. The extracted frame-level (bottleneck, posterior or d-vector) features are equally weighted and aggregated to compute an utterance-level speaker representation (d-vector or i-vector). In this work we use a speaker discriminate CNN to extract the noise-robust frame-level features. These features are smartly combined to form an utterance-level speaker vector through an attention mechanism. The proposed attention model takes the speaker discriminate information and the phonetic information to learn the weights. The whole system, including the CNN and attention model, is joint optimized using an end-to-end criterion. The training algorithm imitates exactly the evaluation process — directly mapping a test utterance and a few target speaker utterances into a single verification score. The algorithm can smartly select the most similar impostor for each target speaker to train the network. We demonstrated the effectiveness of the proposed end-to-end system on Windows 10 “Hey Cortana” speaker verification task.},
author = {Shi-Xiong Zhang and Zhuo Chen and Yong Zhao and Jinyu Li and Y. Gong},
doi = {10.1109/SLT.2016.7846261},
arxivid = {1701.00562},
}

@article{6de99515ff86b42abb1107f920109cbee6037f81,
title = {Unsupervised Methods for Speaker Diarization: An Integrated and Iterative Approach},
year = {2013},
url = {https://www.semanticscholar.org/paper/6de99515ff86b42abb1107f920109cbee6037f81},
abstract = {In speaker diarization, standard approaches typically perform speaker clustering on some initial segmentation before refining the segment boundaries in a re-segmentation step to obtain a final diarization hypothesis. In this paper, we integrate an improved clustering method with an existing re-segmentation algorithm and, in iterative fashion, optimize both speaker cluster assignments and segmentation boundaries jointly. For clustering, we extend our previous research using factor analysis for speaker modeling. In continuing to take advantage of the effectiveness of factor analysis as a front-end for extracting speaker-specific features (i.e., i-vectors), we develop a probabilistic approach to speaker clustering by applying a Bayesian Gaussian Mixture Model (GMM) to principal component analysis (PCA)-processed i-vectors. We then utilize information at different temporal resolutions to arrive at an iterative optimization scheme that, in alternating between clustering and re-segmentation steps, demonstrates the ability to improve both speaker cluster assignments and segmentation boundaries in an unsupervised manner. Our proposed methods attain results that are comparable to those of a state-of-the-art benchmark set on the multi-speaker CallHome telephone corpus. We further compare our system with a Bayesian nonparametric approach to diarization and attempt to reconcile their differences in both methodology and performance.},
author = {Stephen Shum and N. Dehak and Réda Dehak and James R. Glass},
doi = {10.1109/TASL.2013.2264673},
}

@article{32d21dc13f8770958b196a96f99a6f3959c7dc0f,
title = {MUSAN: A Music, Speech, and Noise Corpus},
year = {2015},
url = {https://www.semanticscholar.org/paper/32d21dc13f8770958b196a96f99a6f3959c7dc0f},
abstract = {This report introduces a new corpus of music, speech, and noise. This dataset is suitable for training models for voice activity detection (VAD) and music/speech discrimination. Our corpus is released under a flexible Creative Commons license. The dataset consists of music from several genres, speech from twelve languages, and a wide assortment of technical and non-technical noises. We demonstrate use of this corpus for music/speech discrimination on Broadcast news and VAD for speaker identification.},
author = {David Snyder and Guoguo Chen and Daniel Povey},
arxivid = {1510.08484},
}

@article{a8c3907b09d62457c3b1ebce203e2d9e4af0121e,
title = {Deep neural network-based speaker embeddings for end-to-end speaker verification},
year = {2016},
url = {https://www.semanticscholar.org/paper/a8c3907b09d62457c3b1ebce203e2d9e4af0121e},
abstract = {In this study, we investigate an end-to-end text-independent speaker verification system. The architecture consists of a deep neural network that takes a variable length speech segment and maps it to a speaker embedding. The objective function separates same-speaker and different-speaker pairs, and is reused during verification. Similar systems have recently shown promise for text-dependent verification, but we believe that this is unexplored for the text-independent task. We show that given a large number of training speakers, the proposed system outperforms an i-vector baseline in equal error-rate (EER) and at low miss rates. Relative to the baseline, the end-to-end system reduces EER by 13% average and 29% pooled across test conditions. The fused system achieves a reduction of 32% average and 38% pooled.},
author = {David Snyder and Pegah Ghahremani and Daniel Povey and D. Garcia-Romero and Yishay Carmiel and S. Khudanpur},
doi = {10.1109/SLT.2016.7846260},
}

@article{4626f2ff8be01559d9a1975a651560d0b7da954b,
title = {Text-Independent Speaker Verification Based on Triplet Loss},
year = {2020},
url = {https://www.semanticscholar.org/paper/4626f2ff8be01559d9a1975a651560d0b7da954b},
abstract = {An improved end-to-end text-independent speaker verification model is proposed in this paper. LSTM networks are employed to extract the speaker model embedding, and the triplet loss is used to optimize the training of the network which make the training of the speaker verification model more efficient while keep the computation complexity relatively low. With the triplet loss, the proposed model can achieve better EER performance.},
author = {Junjie He and Jing He and Liangjin Zhu},
doi = {10.1109/ITNEC48623.2020.9085125},
}

@article{369728d7576683a25de8890e4bc02fae6132fccb,
title = {Deep Neural Network Embeddings for Text-Independent Speaker Verification},
year = {2017},
url = {https://www.semanticscholar.org/paper/369728d7576683a25de8890e4bc02fae6132fccb},
abstract = {This paper investigates replacing i-vectors for text-independent speaker verification with embeddings extracted from a feedforward deep neural network. Long-term speaker characteristics are captured in the network by a temporal pooling layer that aggregates over the input speech. This enables the network to be trained to discriminate between speakers from variablelength speech segments. After training, utterances are mapped directly to fixed-dimensional speaker embeddings and pairs of embeddings are scored using a PLDA-based backend. We compare performance with a traditional i-vector baseline on NIST SRE 2010 and 2016. We find that the embeddings outperform i-vectors for short speech segments and are competitive on long duration test conditions. Moreover, the two representations are complementary, and their fusion improves on the baseline at all operating points. Similar systems have recently shown promising results when trained on very large proprietary datasets, but to the best of our knowledge, these are the best results reported for speaker-discriminative neural networks when trained and tested on publicly available corpora.},
author = {David Snyder and D. Garcia-Romero and Daniel Povey and S. Khudanpur},
doi = {10.21437/INTERSPEECH.2017-620},
}

@article{1f010dd1226834f16515a6610ea57b480266ef2d,
title = {Fully Supervised Speaker Diarization},
year = {2018},
url = {https://www.semanticscholar.org/paper/1f010dd1226834f16515a6610ea57b480266ef2d},
abstract = {In this paper, we propose a fully supervised speaker diarization approach, named unbounded interleaved-state recurrent neural networks (UIS-RNN). Given extracted speaker-discriminative embeddings (a.k.a. d-vectors) from input utterances, each individual speaker is modeled by a parameter-sharing RNN, while the RNN states for different speakers interleave in the time domain. This RNN is naturally integrated with a distance-dependent Chinese restaurant process (ddCRP) to accommodate an unknown number of speakers. Our system is fully supervised and is able to learn from examples where time-stamped speaker labels are annotated. We achieved a 7.6% diarization error rate on NIST SRE 2000 CALLHOME, which is better than the state-of-the-art method using spectral clustering. Moreover, our method decodes in an online fashion while most state-of-the-art systems rely on offline clustering.},
author = {Aonan Zhang and Quan Wang and Zhenyao Zhu and J. Paisley and Chong Wang},
doi = {10.1109/ICASSP.2019.8683892},
arxivid = {1810.04719},
}

@article{8fc09dfcff78ac9057ff0834a83d23eb38ca198a,
title = {Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis},
year = {2018},
url = {https://www.semanticscholar.org/paper/8fc09dfcff78ac9057ff0834a83d23eb38ca198a},
abstract = {We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.},
author = {Ye Jia and Yu Zhang and Ron J. Weiss and Quan Wang and Jonathan Shen and Fei Ren and Z. Chen and P. Nguyen and Ruoming Pang and I. Lopez-Moreno and Yonghui Wu},
arxivid = {1806.04558},
}

@article{389cd9824428be98a710f5f4de67121a70c15fd3,
title = {X-Vectors: Robust DNN Embeddings for Speaker Recognition},
year = {2018},
url = {https://www.semanticscholar.org/paper/389cd9824428be98a710f5f4de67121a70c15fd3},
abstract = {In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.},
author = {David Snyder and D. Garcia-Romero and Gregory Sell and Daniel Povey and S. Khudanpur},
doi = {10.1109/ICASSP.2018.8461375},
}

@article{7ba45cd61ae6d7bd6033a18dbd9920337e15f46a,
title = {End-to-end text-dependent speaker verification},
year = {2015},
url = {https://www.semanticscholar.org/paper/7ba45cd61ae6d7bd6033a18dbd9920337e15f46a},
abstract = {In this paper we present a data-driven, integrated approach to speaker verification, which maps a test utterance and a few reference utterances directly to a single score for verification and jointly optimizes the system's components using the same evaluation protocol and metric as at test time. Such an approach will result in simple and efficient systems, requiring little domain-specific knowledge and making few model assumptions. We implement the idea by formulating the problem as a single neural network architecture, including the estimation of a speaker model on only a few utterances, and evaluate it on our internal "Ok Google" benchmark for text-dependent speaker verification. The proposed approach appears to be very effective for big data applications Like ours that require highly accurate, easy-to-maintain systems with a small footprint.},
author = {G. Heigold and Ignacio Moreno and Samy Bengio and Noam M. Shazeer},
doi = {10.1109/ICASSP.2016.7472652},
arxivid = {1509.08062},
}
