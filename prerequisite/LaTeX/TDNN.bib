@article{9609f4817a7e769f5e3e07084db35e46696e82cd,
title = {ECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification},
year = {2020},
url = {https://www.semanticscholar.org/paper/9609f4817a7e769f5e3e07084db35e46696e82cd},
abstract = {Current speaker verification techniques rely on a neural network to extract speaker representations. The successful x-vector architecture is a Time Delay Neural Network (TDNN) that applies statistics pooling to project variable-length utterances into fixed-length speaker characterizing embeddings. In this paper, we propose multiple enhancements to this architecture based on recent trends in the related fields of face verification and computer vision. Firstly, the initial frame layers can be restructured into 1-dimensional Res2Net modules with impactful skip connections. Similarly to SE-ResNet, we introduce Squeeze-and-Excitation blocks in these modules to explicitly model channel interdependencies. The SE block expands the temporal context of the frame layer by rescaling the channels according to global properties of the recording. Secondly, neural networks are known to learn hierarchical features, with each layer operating on a different level of complexity. To leverage this complementary information, we aggregate and propagate features of different hierarchical levels. Finally, we improve the statistics pooling module with channel-dependent frame attention. This enables the network to focus on different subsets of frames during each of the channel's statistics estimation. The proposed ECAPA-TDNN architecture significantly outperforms state-of-the-art TDNN based systems on the VoxCeleb test sets and the 2019 VoxCeleb Speaker Recognition Challenge.},
author = {Brecht Desplanques and Jenthe Thienpondt and Kris Demuynck},
doi = {10.21437/INTERSPEECH.2020-2650},
arxivid = {2005.07143},
}

@article{c82fcaa8977dfb423f32713c607f0bbe17685961,
title = {Selective Kernel Attention for Robust Speaker Verification},
year = {2022},
url = {https://www.semanticscholar.org/paper/c82fcaa8977dfb423f32713c607f0bbe17685961},
abstract = {Recent state-of-the-art speaker veriﬁcation architectures adopt multi-scale processing and frequency-channel attention tech-niques. However, their full potential may not have been ex-ploited because these techniques’ receptive ﬁelds are ﬁxed where most convolutional layers operate with speciﬁed kernel sizes such as 1, 3 or 5. We aim to further improve this line of research by introducing a selective kernel attention (SKA) mechanism. The SKA mechanism allows each convolutional layer to adaptively select the kernel size in a data-driven fashion based on an attention mechanism that exploits both frequency and channel domain using the previous layer’s output. We propose three module variants using the SKA mechanism whereby two modules are applied in front of an ECAPA-TDNN model, and the other is combined with the Res2Net backbone block. Experimental results demonstrate that our proposed model consistently outperforms the conventional counterpart on the three different evaluation protocols in terms of both equal error rate and minimum detection cost function. In addition, we present a detailed analysis that helps understand how the SKA module works.},
author = {Sung Hwan Mun and Jee-weon Jung and N. Kim},
journal = {ArXiv},
volume = {abs/2204.01005},
pages = {null},
doi = {10.48550/arXiv.2204.01005},
}

@article{4a64008cd21488071ac418aad32ce7129408edb4,
title = {Attentive Statistics Pooling for Deep Speaker Embedding},
year = {2018},
url = {https://www.semanticscholar.org/paper/4a64008cd21488071ac418aad32ce7129408edb4},
abstract = {This paper proposes attentive statistics pooling for deep speaker embedding in text-independent speaker verification. In conventional speaker embedding, frame-level features are averaged over all the frames of a single utterance to form an utterance-level feature. Our method utilizes an attention mechanism to give different weights to different frames and generates not only weighted means but also weighted standard deviations. In this way, it can capture long-term variations in speaker characteristics more effectively. An evaluation on the NIST SRE 2012 and the VoxCeleb data sets shows that it reduces equal error rates (EERs) from the conventional method by 7.5% and 8.1%, respectively.},
author = {K. Okabe and Takafumi Koshinaka and K. Shinoda},
journal = {ArXiv},
volume = {abs/1803.10963},
pages = {2252-2256},
doi = {10.21437/Interspeech.2018-993},
arxivid = {1803.10963},
}

@article{185c5fdc6604d49249388742f3b9e08dfa1d873f,
title = {CS-CTCSCONV1D: Small footprint speaker verification with channel split time-channel-time separable 1-dimensional convolution},
year = {2022},
url = {https://www.semanticscholar.org/paper/185c5fdc6604d49249388742f3b9e08dfa1d873f},
abstract = {We present an efficient small-footprint network for speaker verification. We start by introducing the bottleneck to the QuartzNet model. Then we proposed a Channel Split Time-Channel-Time Separable 1-dimensional Convolution (CS-CTCSConv1d) module, yielding stronger performance over the State-Of-The-Art small footprint speaker verification system. We apply knowledge distillation to further improve performance to learn better speaker embedding from the large model. We evaluate the proposed approach on Voxceleb dataset, obtain-ing better performances concerning the baseline method. The proposed model takes only 238.9K parameters to outperform the baseline system by 10% relatively in equal error rate (EER).},
author = {Linjun Cai and Yuhong Yang and Xufeng Chen and Weiping Tu and Hongyang Chen},
journal = {Interspeech 2022},
volume = {null},
pages = {326-330},
doi = {10.21437/interspeech.2022-913},
}

@article{1b4de223d5393bb2abac0ecb6e46de0a020ad18e,
title = {Improving Time Delay Neural Network Based Speaker Recognition with Convolutional Block and Feature Aggregation Methods},
year = {2021},
url = {https://www.semanticscholar.org/paper/1b4de223d5393bb2abac0ecb6e46de0a020ad18e},
abstract = {In this paper, we develop a system that integrates multiple ideas and techniques inspired by the convolutional block and feature aggregation methods. We begin with the stateof-the-art speaker-embedding model for speaker recognition, namely the model of Emphasized Channel Attention, Propagation, and Aggregation in Time Delay Neural Network, and then gradually experiment with the proposed network modules, including bottleneck residual blocks, attention mechanisms, and feature aggregation methods. In our final model, we replace the Res2Block with SC-Block and we use a hierarchical architecture for feature aggregation. We evaluate the performance of our model on the VoxCeleb1 test set and the 2020 VoxCeleb Speaker Recognition Challenge (VoxSRC20) validation set. The relative improvement of the proposed models over ECAPA-TDNN is 22.8% on VoxCeleb1 and 18.2% on VoxSRC20.},
author = {Yu-Jia Zhang and Yih-Wen Wang and Chia-Ping Chen and Chung-Li Lu and Bo-Cheng Chan},
doi = {10.21437/interspeech.2021-356},
}

@article{f56e9b5622141de6319e6bca39ab0ad235ef89e5,
title = {Large-Scale Self-Supervised Speech Representation Learning for Automatic Speaker Verification},
year = {2021},
url = {https://www.semanticscholar.org/paper/f56e9b5622141de6319e6bca39ab0ad235ef89e5},
abstract = {The speech representations learned from large-scale unlabeled data have shown better generalizability than those from supervised learning and thus attract a lot of interest to be applied for various downstream tasks. In this paper, we explore the limits of speech representations learned by different self-supervised objectives and datasets for automatic speaker verification (ASV), especially with a well-recognized SOTA ASV model, ECAPA-TDNN [1], as a downstream model. The representations from all hidden layers of the pre-trained model are firstly averaged with learnable weights and then fed into the ECAPA-TDNN as input features. The experimental results on Voxceleb dataset show that the weighted average representation is significantly superior to FBank, a conventional handcrafted feature for ASV. Our best single system achieves 0.537%, 0.569%, and 1.180% equal error rate (EER) on the three official trials of VoxCeleb1, separately. Accordingly, the ensemble system with three pre-trained models can further improve the EER to 0.479%, 0.536% and 1.023%. Among the three evaluation trials, our best system outperforms the winner system [2] of the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC2021) on the VoxCeleb1-E trial.},
author = {Zhengyang Chen and Sanyuan Chen and Yu Wu and Yao Qian and Chengyi Wang and Shujie Liu and Y. Qian and Michael Zeng},
journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {6147-6151},
doi = {10.1109/icassp43922.2022.9747814},
arxivid = {2110.05777},
}

@article{899172bf6b54aeb4359fcf6c18d50b63358f34b6,
title = {Improving Deep CNN Architectures with Variable-Length Training Samples for Text-Independent Speaker Verification},
year = {2021},
url = {https://www.semanticscholar.org/paper/899172bf6b54aeb4359fcf6c18d50b63358f34b6},
abstract = {Deep Convolutional Neural Network (CNN) based speaker embeddings, such as r-vectors, have shown great success in text-independent speaker veriﬁcation (TI-SV) task. However, previous deep CNN models usually use ﬁxed-length samples for training and employ variable-length utterances for speaker embeddings, which generates a mismatch between training and embedding. To address this issue, we investigate the effect of employing variable-length training samples on CNN-based TI-SV systems and explore two approaches to improve the performance of deep CNN architectures on TI-SV through capturing variable-term contexts. Firstly, we present an improved selective kernel convolution which allows the networks to adaptively switch between short-term and long-term contexts based on variable-length utterances. Secondly, we propose a multi-scale statistics pooling method to aggregate multiple time-scale features from different layers of the networks. We build a novel ResNet34 based architecture with two proposed approaches. Experiments are conducted on the VoxCeleb datasets. The results demonstrate that the effect of using variable-length samples is diverse in different networks and the architecture with two proposed approaches achieves signiﬁcant improvement over r-vectors baseline system.},
author = {Yanfeng Wu and Junan Zhao and Chenkai Guo and Jingjing Xu},
doi = {10.21437/interspeech.2021-559},
}

@article{e679eace726c886c85bd463d6b62a86d51f892b6,
title = {Our Learned Lessons from Cross-Lingual Speaker Verification: The CRMI-DKU System Description for the Short-Duration Speaker Verification Challenge 2021},
year = {2021},
url = {https://www.semanticscholar.org/paper/e679eace726c886c85bd463d6b62a86d51f892b6},
abstract = {In this paper, we present our CRMI-DKU system description for the Short-duration Speaker Veri ﬁ cation Challenge (SdSVC) 2021. We introduce the whole pipeline of our cross-lingual speaker veri ﬁ cation system, including data preprocess-ing, training strategy, utterance-level speaker embedding extractor, domain-adaptation, and score calibration. We also propose methods to learn language-invariant features and perform domain adaptation to reduce the cross-lingual mismatch. In addi-tion, we explore a semi-supervised method to utilize the unlabeled training data. The ﬁ nal submitted score level fusion system achieves 0.0476 minDCF and 0.98% EER on the evaluation set.},
author = {Xiaoyi Qin and Chao Wang and Yong Ma and Min Liu and Shilei Zhang and Ming Li},
doi = {10.21437/interspeech.2021-398},
}

@article{8bdc0d1649f37b369ef727f005959b3b665da604,
title = {The SJTU System for Short-Duration Speaker Verification Challenge 2021},
year = {2021},
url = {https://www.semanticscholar.org/paper/8bdc0d1649f37b369ef727f005959b3b665da604},
abstract = {This paper presents the SJTU system for both text-dependent and text-independent tasks in short-duration speaker veriﬁcation (SdSV) challenge 2021. In this challenge, we explored different strong embedding extractors to extract robust speaker embedding. For text-independent task, language-dependent adaptive snorm is explored to improve the system performance under the cross-lingual veriﬁcation condition. For text-dependent task, we mainly focus on the in-domain ﬁne-tuning strategies based on the model pre-trained on large-scale out-of-domain data. In order to improve the distinction between different speakers uttering the same phrase, we proposed several novel phrase-aware ﬁne-tuning strategies and phrase-aware neural PLDA. With such strategies, the system performance is further improved. Finally, we fused the scores of different systems, and our fusion systems achieved 0.0473 in Task1 (rank 3) and 0.0581 in Task2 (rank 8) on the primary evaluation metric.},
author = {Bing Han and Zhengyang Chen and Zhikai Zhou and Y. Qian},
journal = {ArXiv},
volume = {abs/2208.01933},
pages = {null},
doi = {10.21437/interspeech.2021-965},
arxivid = {2208.01933},
}

@article{55a51f80d3c1140f37974715f4b94037f583f5ce,
title = {The ins and outs of speaker recognition: lessons from VoxSRC 2020},
year = {2020},
url = {https://www.semanticscholar.org/paper/55a51f80d3c1140f37974715f4b94037f583f5ce},
abstract = {The VoxCeleb Speaker Recognition Challenge (VoxSRC) at Interspeech 2020 offers a challenging evaluation for speaker recognition systems, which includes celebrities playing different parts in movies. The goal of this work is robust speaker recognition of utterances recorded in these challenging environments. We utilise variants of the popular ResNet architecture for speaker recognition and perform extensive experiments using a range of loss functions and training parameters. To this end, we optimise an efficient training framework that allows powerful models to be trained with limited time and resources. Our trained models demonstrate improvements over most existing works with lighter models and a simple pipeline. The paper shares the lessons learned from our participation in the challenge.},
author = {Yoohwan Kwon and Hee-Soo Heo and Bong-Jin Lee and Joon Son Chung},
journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {5809-5813},
doi = {10.1109/ICASSP39728.2021.9413948},
arxivid = {2010.15809},
}

@article{19b2749cc0f95d6522a84a3039aea315d92ae9fb,
title = {Beijing ZKJ-NPU Speaker Verification System for VoxCeleb Speaker Recognition Challenge 2021},
year = {2021},
url = {https://www.semanticscholar.org/paper/19b2749cc0f95d6522a84a3039aea315d92ae9fb},
abstract = {In this report, we describe the Beijing ZKJ-NPU team submission to the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC-21). We participated in the fully supervised speaker verification track 1 and track 2. In the challenge, we explored various kinds of advanced neural network structures with different pooling layers and objective loss functions. In addition, we introduced the ResNet-DTCF, CoAtNet and PyConv networks to advance the performance of CNN-based speaker embedding model. Moreover, we applied embedding normalization and score normalization at the evaluation stage. By fusing 11 and 14 systems, our final best performances (minDCF/EER) on the evaluation trails are 0.1205/2.8160% and 0.1175/2.8400% respectively for track 1 and 2. With our submission, we came to the second place in the challenge for both tracks.},
author = {Li Zhang and Huan Zhao and Qinling Meng and Yanli Chen and Min Liu and Lei Xie},
journal = {ArXiv},
volume = {abs/2109.03568},
pages = {null},
arxivid = {2109.03568},
}

@article{1ea7cdbc2e30e6f943691a52cbf49f0f632cdd47,
title = {Speaker Verification in Multi-Speaker Environments Using Temporal Feature Fusion},
year = {2022},
url = {https://www.semanticscholar.org/paper/1ea7cdbc2e30e6f943691a52cbf49f0f632cdd47},
abstract = {Verifying the identity of a speaker is crucial in modern human-machine interfaces, e.g., to ensure privacy protection or to enable biometric authentication. Classical speaker verification (SV) approaches estimate a fixed-dimensional embedding from a speech utterance that encodes the speaker's voice characteristics. A speaker is verified if his/her voice embedding is sufficiently similar to the embedding of the claimed speaker. However, such approaches assume that only a single speaker exists in the input. The presence of concurrent speakers is likely to have detrimental effects on the performance. To address SV in a multi-speaker environment, we propose an end-to-end deep learning-based SV system that detects whether the target speaker exists within an input or not. First, an embedding is estimated from a reference utterance to represent the target's characteristics. Second, frame-level features are estimated from the input mixture. The reference embedding is then fused frame-wise with the mixture's features to allow distinguishing the target from other speakers on a frame basis. Finally, the fused features are used to predict whether the target speaker is active in the speech segment or not. Experimental evaluation shows that the proposed method outperforms the x-vector in multi-speaker conditions.},
author = {Ahmad Aloradi and Wolfgang Mack and Mohamed Elminshawi and Emanuël Habets},
journal = {2022 30th European Signal Processing Conference (EUSIPCO)},
volume = {abs/2206.13808},
pages = {354-358},
doi = {10.48550/arXiv.2206.13808},
arxivid = {2206.13808},
}

@article{d7de3f4d5bd36df1c3d05d7c9da8da3b923b2ced,
title = {In defence of metric learning for speaker recognition},
year = {2020},
url = {https://www.semanticscholar.org/paper/d7de3f4d5bd36df1c3d05d7c9da8da3b923b2ced},
abstract = {The objective of this paper is 'open-set' speaker recognition of unseen speakers, where ideal embeddings should be able to condense information into a compact utterance-level representation that has small intra-speaker and large inter-speaker distance. 
A popular belief in speaker recognition is that networks trained with classification objectives outperform metric learning methods. In this paper, we present an extensive evaluation of most popular loss functions for speaker recognition on the VoxCeleb dataset. We demonstrate that the vanilla triplet loss shows competitive performance compared to classification-based losses, and those trained with our proposed metric learning objective outperform state-of-the-art methods.},
author = {Joon Son Chung and Jaesung Huh and Seongkyu Mun and Minjae Lee and Hee-Soo Heo and Soyeon Choe and Chiheon Ham and Sung-Ye Jung and Bong-Jin Lee and Icksang Han},
journal = {ArXiv},
volume = {abs/2003.11982},
pages = {null},
doi = {10.21437/interspeech.2020-1064},
arxivid = {2003.11982},
}

@article{4c80a6854a72fea53bdfaa637c1cc818b1ff7071,
title = {Tongji University Team for the VoxCeleb Speaker Recognition Challenge 2020},
year = {2020},
url = {https://www.semanticscholar.org/paper/4c80a6854a72fea53bdfaa637c1cc818b1ff7071},
abstract = {In this report, we describe the submission of Tongji University team to the CLOSE track of the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020 at Interspeech 2020. We investigate different speaker recognition systems based on the popular ResNet-34 architecture, and train multiple variants via various loss functions. Both Offline and online data augmentation are introduced to improve the diversity of the training set, and score normalization with the exhaustive grid search is applied in the post-processing. Our best fusion of five selected systems for the CLOSE track achieves 0.2800 DCF and 4.7770% EER on the challenge.},
author = {Rui Wang and Zhihua Wei and Yibin Zhan and Zhuoxiao Chen},
journal = {ArXiv},
volume = {abs/2010.08179},
pages = {null},
arxivid = {2010.08179},
}

@article{9a25a9476688026a18050c7475c7a1492f7915d0,
title = {Integrating Frequency Translational Invariance in TDNNs and Frequency Positional Information in 2D ResNets to Enhance Speaker Verification},
year = {2021},
url = {https://www.semanticscholar.org/paper/9a25a9476688026a18050c7475c7a1492f7915d0},
abstract = {This paper describes the IDLab submission for the text-independent task of the Short-duration Speaker Veriﬁcation Challenge 2021 (SdSVC-21). This speaker veriﬁcation compe-tition focuses on short duration test recordings and cross-lingual trials, along with the constraint of limited availability of in-domain DeepMine Farsi training data. Currently, both Time Delay Neural Networks (TDNNs) and ResNets achieve state-of-the-art results in speaker veriﬁcation. These architectures are structurally very different and the construction of hybrid networks looks a promising way forward. We introduce a 2D convolutional stem in a strong ECAPA-TDNN baseline to transfer some of the strong characteristics of a ResNet based model to this hybrid CNN-TDNN architecture. Similarly, we incorporate absolute frequency positional encodings in an SE-ResNet34 architecture. These learnable feature map biases along the frequency axis offer this architecture a straightforward way to exploit frequency positional information. We also propose a frequency-wise variant of Squeeze-Excitation (SE) which better preserves frequency-speciﬁc information when rescaling the feature maps. Both modiﬁed architectures signiﬁcantly outper-form their corresponding baseline on the SdSVC-21 evaluation data and the original VoxCeleb1 test set. A four system fusion containing the two improved architectures achieved a third place in the ﬁnal SdSVC-21 Task 2 ranking.},
author = {Jenthe Thienpondt and Brecht Desplanques and Kris Demuynck},
doi = {10.21437/Interspeech.2021-1570},
arxivid = {2104.02370},
}

@article{be913bd97a8071e7550140249d3b9e73d1d58d63,
title = {Densely Connected Time Delay Neural Network for Speaker Verification},
year = {2020},
url = {https://www.semanticscholar.org/paper/be913bd97a8071e7550140249d3b9e73d1d58d63},
abstract = {Time delay neural network (TDNN) has been widely used in speaker verification tasks. Recently, two TDNN-based models, including extended TDNN (E-TDNN) and factorized TDNN (F-TDNN), are proposed to improve the accuracy of vanilla TDNN. But E-TDNN and F-TDNN increase the number of parameters due to deeper networks, compared with vanilla TDNN. In this paper, we propose a novel TDNN-based model, called densely connected TDNN (D-TDNN), by adopting bottleneck layers and dense connectivity. D-TDNN has fewer parameters than existing TDNN-based models. Furthermore, we propose an improved variant of D-TDNN, called D-TDNN-SS, to employ multiple TDNN branches with short-term and longterm contexts. D-TDNN-SS can integrate the information from multiple TDNN branches with a newly designed channel-wise selection mechanism called statistics-and-selection (SS). Experiments on VoxCeleb datasets show that both D-TDNN and D-TDNN-SS can outperform existing models to achieve stateof-the-art accuracy with fewer parameters, and D-TDNN-SS can achieve better accuracy than D-TDNN.},
author = {Ya-Qi Yu and Wu-Jun Li},
doi = {10.21437/interspeech.2020-1275},
}

@article{8380621af6ef66daf958ac88b75f9142607956e1,
title = {BUT System Description to VoxCeleb Speaker Recognition Challenge 2019},
year = {2019},
url = {https://www.semanticscholar.org/paper/8380621af6ef66daf958ac88b75f9142607956e1},
abstract = {In this report, we describe the submission of Brno University of Technology (BUT) team to the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2019. We also provide a brief analysis of different systems on VoxCeleb-1 test sets. Submitted systems for both Fixed and Open conditions are a fusion of 4 Convolutional Neural Network (CNN) topologies. The first and second networks have ResNet34 topology and use two-dimensional CNNs. The last two networks are one-dimensional CNN and are based on the x-vector extraction topology. Some of the networks are fine-tuned using additive margin angular softmax. Kaldi FBanks and Kaldi PLPs were used as features. The difference between Fixed and Open systems lies in the used training data and fusion strategy. The best systems for Fixed and Open conditions achieved 1.42% and 1.26% ERR on the challenge evaluation set respectively.},
author = {Hossein Zeinali and Shuai Wang and Anna Silnova and P. Matejka and Oldrich Plchot},
journal = {ArXiv},
volume = {abs/1910.12592},
pages = {null},
arxivid = {1910.12592},
}

@article{29abbaef73524207732d2a5091aec2f8712425c9,
title = {The TalTech Systems for the Short-Duration Speaker Verification Challenge 2020},
year = {2020},
url = {https://www.semanticscholar.org/paper/29abbaef73524207732d2a5091aec2f8712425c9},
abstract = {This paper presents the Tallinn University of Technology systems submitted to the Short-duration Speaker Verification Challenge 2020. The challenge consists of two tasks, focusing on text-dependent and text-independent speaker verification with some cross-lingual aspects. We used speaker embedding models that consist of squeeze-and-attention based residual layers, multi-head attention and either cross-entropy-based or additive angular margin based objective function. In order to encourage the model to produce language-independent embeddings, we trained the models in a multi-task manner, using dataset specific output layers. In the text-dependent task we employed a phrase classifier to reject trials with non-matching phrases. In the text-independent task we used a language classifier to boost the scores of trials where the language of the test and enrollment utterances does not match. Our final primary metric score was 0.075 in Task 1 (ranked as 6th) and 0.118 in Task 2 (rank 8).},
author = {Tanel Alumäe and Jörgen Valk},
doi = {10.21437/interspeech.2020-2233},
}

@article{dbdd9de12af100935f4dc19b7ff46159a9a4aff1,
title = {MACCIF-TDNN: Multi Aspect Aggregation of Channel and Context Interdependence Features in TDNN-Based Speaker Verification},
year = {2021},
url = {https://www.semanticscholar.org/paper/dbdd9de12af100935f4dc19b7ff46159a9a4aff1},
abstract = {Most of the recent state-of-the-art results for speaker verification are achieved by X-vector and its subsequent variants. In this paper, we propose a new network architecture which aggregates the channel and context interdependence features from multi aspects based on Time Delay Neural Network (TDNN). Firstly, we use the SE-Res2Blocks as in ECAPA-TDNN to explicitly model the channel interdependence to realize adaptive calibration of channel features, and process local context features in a multi-scale way at a more granular level compared with conventional TDNN-based methods. Secondly, we explore to use the encoder structure of Transformer to model the global context interdependence features at an utterance level which can capture better long term temporal characteristics. Before the pooling layer, we aggregate the outputs of SE-Res2Blocks and Transformer Encoders to leverage the complementary channel and context interdependence features learned by themself respectively. Finally, instead of performing a single attentive statistics pooling, we also find it beneficial to extend the pooling method in a multi-head way which can discriminate features from multiple aspects. The proposed MACCIF-TDNN architecture can outperform most of the state-of-the-art TDNN based systems on VoxCeleb1 test sets.},
author = {Fangyuan Wang and Z. Song and Hongchen Jiang and Bo Xu},
journal = {2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)},
volume = {null},
pages = {214-219},
doi = {10.1109/ASRU51503.2021.9687918},
arxivid = {2107.03104},
}

@article{52c93565f798a296e5016b17b1a7a37fbbde750e,
title = {Convolution-Based Channel-Frequency Attention for Text-Independent Speaker Verification},
year = {2022},
url = {https://www.semanticscholar.org/paper/52c93565f798a296e5016b17b1a7a37fbbde750e},
abstract = {Deep convolutional neural networks (CNNs) have been applied to extracting speaker embeddings with signiﬁcant success in speaker veriﬁcation. Incorporating the attention mechanism has shown to be effective in improving the model performance. This paper presents an efﬁcient two-dimensional convolution-based attention module, namely C2D-Att . The interaction between the convolution channel and frequency is involved in the attention calculation by lightweight convolution layers. This requires only a small number of parameters. Fine-grained attention weights are produced to represent channel and frequency-speciﬁc information. The weights are imposed on the input features to improve the representation ability for speaker modeling. The C2D-Att is integrated into a modiﬁed version of ResNet for speaker embedding extraction. Experiments are conducted on VoxCeleb datasets. The results show that C2D-Att is effective in generating discriminative attention maps and outperforms other attention methods. The proposed model shows robust performance with different scales of model size and achieves state-of-the-art results.},
author = {Jingyu Li and Yusheng Tian and Tan Lee},
journal = {ArXiv},
volume = {abs/2210.17310},
pages = {null},
doi = {10.48550/arXiv.2210.17310},
arxivid = {2210.17310},
}

@article{62a007787bdf51bb58668d2a88df18850c4e9e28,
title = {Spine2Net: SpineNet with Res2Net and Time-Squeeze-and-Excitation Blocks for Speaker Recognition},
year = {2021},
url = {https://www.semanticscholar.org/paper/62a007787bdf51bb58668d2a88df18850c4e9e28},
abstract = {Modeling speaker embeddings using deep neural networks is currently state-of-the-art in speaker recognition. Recently, ResNet-based structures have gained a broader interest, slowly becoming the baseline along with the deep-rooted Time Delay Neural Network based models. However, the scale-decreased design of the ResNet models may not preserve all of the speaker information. In this paper, we investigate the SpineNet structure with scale-permuted design to tackle this problem, in which feature size either increases or decreases depending on the processing stage in the network. Apart from the presented adjustments of the SpineNet model for the speaker recognition task, we also incorporate popular modules dedicated to the residual-like structures, namely the Res2Net and Squeeze-and-Excitation blocks, and modify them to work effectively in the presented neural network architectures. The ﬁnal proposed model, i.e., the SpineNet architecture with Res2Net and Time-Squeeze-and-Excitation blocks, achieves remarkable Equal Error Rates of 0.99 and 0.92 for the Extended and Original trial lists of the well-known VoxCeleb1 dataset.},
author = {Magdalena Rybicka and J. Villalba and Piotr Żelasko and N. Dehak and K. Kowalczyk},
doi = {10.21437/interspeech.2021-1163},
}

@article{aa3ba08a4451871da8a2e92ab11b3ed26c292bcf,
title = {The JHU Speaker Recognition System for the VOiCES 2019 Challenge},
year = {2019},
url = {https://www.semanticscholar.org/paper/aa3ba08a4451871da8a2e92ab11b3ed26c292bcf},
abstract = {This paper describes the systems developed by the JHU team for the speaker recognition track of the 2019 VOiCES from a Distance Challenge. On this far-field task, we achieved good performance using systems based on state-of-the-art deep neural network (DNN) embeddings. In this paradigm, a DNN maps variable-length speech segments to speaker embeddings, called x-vectors, that are then classified using probabilistic linear discriminant analysis (PLDA). Our submissions were composed of three x-vector-based systems that differed primarily in the DNN architecture, temporal pooling mechanism, and training objective function. On the evaluation set, our best single-system submission used an extended time-delay architecture, and achieved 0.435 in actual DCF, the primary evaluation metric. A fusion of all three x-vector systems was our primary submission, and it obtained an actual DCF of 0.362.},
author = {David Snyder and J. Villalba and Nanxin Chen and Daniel Povey and Gregory Sell and N. Dehak and S. Khudanpur},
doi = {10.21437/interspeech.2019-2979},
}

@article{15335294310b14f9e86de75085b991249557fc6d,
title = {The Idlab Voxsrc-20 Submission: Large Margin Fine-Tuning and Quality-Aware Score Calibration in DNN Based Speaker Verification},
year = {2020},
url = {https://www.semanticscholar.org/paper/15335294310b14f9e86de75085b991249557fc6d},
abstract = {In this paper we propose and analyse a large margin fine-tuning strategy and a quality-aware score calibration in text-independent speaker verification. Large margin fine-tuning is a secondary training stage for DNN based speaker verification systems trained with margin-based loss functions. It enables the network to create more robust speaker embeddings by enabling the use of longer training utterances in combination with a more aggressive margin penalty. Score calibration is a common practice in speaker verification systems to map output scores to well-calibrated log-likelihood-ratios, which can be converted to interpretable probabilities. By including quality features in the calibration system, the decision thresholds of the evaluation metrics become quality-dependent and more consistent across varying trial conditions. Applying both enhancements on the ECAPA-TDNN architecture leads to state-of-the-art results on all publicly available VoxCeleb1 test sets and contributed to our winning submissions in the supervised verification tracks of the Vox-Celeb Speaker Recognition Challenge 2020.},
author = {Jenthe Thienpondt and Brecht Desplanques and Kris Demuynck},
journal = {ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {5814-5818},
doi = {10.1109/ICASSP39728.2021.9414600},
arxivid = {2010.11255},
}

@article{5299bb0103193b4232ce9f4b0f609a6c7839c847,
title = {Tackling the Score Shift in Cross-Lingual Speaker Verification by Exploiting Language Information},
year = {2021},
url = {https://www.semanticscholar.org/paper/5299bb0103193b4232ce9f4b0f609a6c7839c847},
abstract = {This paper contains a post-challenge performance analysis on cross-lingual speaker verification of the IDLab submission to the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC-21). We show that current speaker embedding extractors consistently underestimate speaker similarity in within-speaker cross-lingual trials. Consequently, the typical training and scoring protocols do not put enough emphasis on the compensation of intra-speaker language variability. We propose two techniques to increase cross-lingual speaker verification robustness. First, we enhance our previously proposed Large-Margin Fine-Tuning (LM-FT) training stage with a mini-batch sampling strategy which increases the amount of intra-speaker cross-lingual samples within the mini-batch. Second, we incorporate language information in the logistic regression calibration stage. We integrate quality metrics based on soft and hard decisions of a VoxLingua107 language identification model. The proposed techniques result in a 11.7% relative improvement over the baseline model on the VoxSRC-21 test set and contributed to our third place finish in the corresponding challenge.},
author = {Jenthe Thienpondt and Brecht Desplanques and Kris Demuynck},
journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {7187-7191},
doi = {10.1109/ICASSP43922.2022.9746210},
arxivid = {2110.09150},
}

@article{cc1f33da9975a48d3add8983aaf081e738aae4f6,
title = {Channel Interdependence Enhanced Speaker Embeddings for Far-Field Speaker Verification},
year = {2021},
url = {https://www.semanticscholar.org/paper/cc1f33da9975a48d3add8983aaf081e738aae4f6},
abstract = {Recognizing speakers from a distance using far-field microphones is difficult because of the environmental noise and reverberation distortion. In this work, we tackle these problems by strengthening the frame-level processing and feature aggregation of x-vector networks. Specifically, we restructure the dilated convolutional layers into Res2Net blocks to generate multi-scale frame-level features. To exploit the relationship between the channels, we introduce squeeze-and-excitation (SE) units to rescale the channels’ activations and investigate the best places to put these SE units in the Res2Net blocks. Based on the hypothesis that layers at different depth contain speaker information at different granularity levels, multi-block feature aggregation is introduced to propagate and aggregate the features at various depths. To optimally weight the channels and frames during feature aggregation, we propose a channel-dependent attention mechanism. Combining all of these enhancements leads to a network architecture called channel-interdependence enhanced Res2Net (CE-Res2Net). Results show that the proposed network achieves a relative improvement of about 16% in EER and 17% in minDCF on the VOiCES 2019 Challenge’s evaluation set.},
author = {Ling-jun Zhao and M. Mak},
journal = {2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP)},
volume = {null},
pages = {1-5},
doi = {10.1109/ISCSLP49672.2021.9362108},
}

@article{ca150904327f3cf5792c4cd1440a8f0fe926a3da,
title = {Clova Baseline System for the VoxCeleb Speaker Recognition Challenge 2020},
year = {2020},
url = {https://www.semanticscholar.org/paper/ca150904327f3cf5792c4cd1440a8f0fe926a3da},
abstract = {This report describes our submission to the VoxCeleb Speaker Recognition Challenge (VoxSRC) at Interspeech 2020. We perform a careful analysis of speaker recognition models based on the popular ResNet architecture, and train a number of variants using a range of loss functions. Our results show significant improvements over most existing works without the use of model ensemble or post-processing. We release the training code and pre-trained models as unofficial baselines for this year's challenge.},
author = {Hee-Soo Heo and Bong-Jin Lee and Jaesung Huh and Joon Son Chung},
journal = {ArXiv},
volume = {abs/2009.14153},
pages = {null},
arxivid = {2009.14153},
}

@article{a1c6e6774e3dfcd8c1cffd5493a359ad373e6ffa,
title = {Cross Attentive Pooling for Speaker Verification},
year = {2020},
url = {https://www.semanticscholar.org/paper/a1c6e6774e3dfcd8c1cffd5493a359ad373e6ffa},
abstract = {The goal of this paper is text-independent speaker verification where utterances come from ‘in the wild’ videos and may contain irrelevant signal. While speaker verification is naturally a pair-wise problem, existing methods to produce the speaker embeddings are instance-wise. In this paper, we propose Cross Attentive Pooling (CAP) that utilises the context information across the reference-query pair to generate utterance-level embeddings that contain the most discriminative information for the pair-wise matching problem. Experiments are performed on the VoxCeleb dataset in which our method outperforms comparable pooling strategies.},
author = {Seong Min Kye and Yoohwan Kwon and Joon Son Chung},
journal = {2021 IEEE Spoken Language Technology Workshop (SLT)},
volume = {null},
pages = {294-300},
doi = {10.1109/SLT48900.2021.9383565},
arxivid = {2008.05983},
}

@article{b8dd289b9aa24a5df4e0cf254255bbeddf1678a3,
title = {XMU-TS Systems for NIST SRE19 CTS Challenge},
year = {2020},
url = {https://www.semanticscholar.org/paper/b8dd289b9aa24a5df4e0cf254255bbeddf1678a3},
abstract = {In this paper, we present our submitted XMU-TS system for NIST SRE19 CTS Challenge. The evaluation of this year only offers the open training condition. With the large amounts of data assimilated into training set, the diversity of training data sources inevitably leads to domain mismatch, which becomes a key factor affecting the system performance. In order to solve this problem, we have made a lot of attempts. Based on the x-vector framework, we used different network structures, and tried to modify the performance of factorized time delay deep neural network (F-TDNN) and residual network (ResNet). In addition, in the back-end classifier, we used domain adaption to eliminate the impact of domain mismatch. Finally, we employed Adaptive Symmetric Score Normalization (AS-Norm) for score normalization to adjust the fractional distribution space. These attempts have enriched the diversity of our systems, enabling the fusion system to complement each subsystem and improve the final submission performace.},
author = {Hao Lu and Jianfeng Zhou and Miao Zhao and Wendian Lei and Q. Hong and Lin Li},
journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {7569-7573},
doi = {10.1109/ICASSP40776.2020.9053080},
}

@article{8875ae233bc074f5cd6c4ebba447b536a7e847a5,
title = {VoxCeleb2: Deep Speaker Recognition},
year = {2018},
url = {https://www.semanticscholar.org/paper/8875ae233bc074f5cd6c4ebba447b536a7e847a5},
abstract = {The objective of this paper is speaker recognition under noisy and unconstrained conditions. 
We make two key contributions. First, we introduce a very large-scale audio-visual speaker recognition dataset collected from open-source media. Using a fully automated pipeline, we curate VoxCeleb2 which contains over a million utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset. 
Second, we develop and compare Convolutional Neural Network (CNN) models and training strategies that can effectively recognise identities from voice under various conditions. The models trained on the VoxCeleb2 dataset surpass the performance of previous works on a benchmark dataset by a significant margin.},
author = {Joon Son Chung and Arsha Nagrani and Andrew Zisserman},
doi = {10.21437/Interspeech.2018-1929},
arxivid = {1806.05622},
}

@article{31504ac4057b02b9fe500d8cb994f27d8ab1484b,
title = {The IDLAB VoxCeleb Speaker Recognition Challenge 2020 System Description},
year = {2020},
url = {https://www.semanticscholar.org/paper/31504ac4057b02b9fe500d8cb994f27d8ab1484b},
abstract = {In this technical report we describe the IDLAB top-scoring submissions for the VoxCeleb Speaker Recognition Challenge 2020 (VoxSRC-20) in the supervised and unsupervised speaker verification tracks. For the supervised verification tracks we trained 6 state-of-the-art ECAPA-TDNN systems and 4 Resnet34 based systems with architectural variations. On all models we apply a large margin fine-tuning strategy, which enables the training procedure to use higher margin penalties by using longer training utterances. In addition, we use quality-aware score calibration which introduces quality metrics in the calibration system to generate more consistent scores across varying levels of utterance conditions. A fusion of all systems with both enhancements applied led to the first place on the open and closed supervised verification tracks. The unsupervised system is trained through contrastive learning. Subsequent pseudo-label generation by iterative clustering of the training embeddings allows the use of supervised techniques. This procedure led to the winning submission on the unsupervised track, and its performance is closing in on supervised training.},
author = {Jenthe Thienpondt and Brecht Desplanques and Kris Demuynck},
journal = {ArXiv},
volume = {abs/2109.04070},
pages = {null},
arxivid = {2109.04070},
}

@article{3c1533d3710d34680690f99498719a0bddb6d88d,
title = {ECAPA-TDNN Embeddings for Speaker Diarization},
year = {2021},
url = {https://www.semanticscholar.org/paper/3c1533d3710d34680690f99498719a0bddb6d88d},
abstract = {Learning robust speaker embeddings is a crucial step in speaker diarization. Deep neural networks can accurately capture speaker discriminative characteristics and popular deep embeddings such as x-vectors are nowadays a fundamental component of modern diarization systems. Recently, some improvements over the standard TDNN architecture used for x-vectors have been proposed. The ECAPA-TDNN model, for instance, has shown impressive performance in the speaker veriﬁcation domain, thanks to a carefully designed neural model. In this work, we extend, for the ﬁrst time, the use of the ECAPA-TDNN model to speaker diarization. Moreover, we improved its robustness with a powerful augmentation scheme that concatenates several contaminated versions of the same signal within the same training batch. The ECAPA-TDNN model turned out to provide robust speaker embeddings under both close-talking and distant-talking conditions. Our results on the popular AMI meeting corpus show that our system signiﬁcantly outperforms recently proposed approaches.},
author = {Nauman Dawalatabad and M. Ravanelli and Franccois Grondin and Jenthe Thienpondt and Brecht Desplanques and Hwidong Na},
doi = {10.21437/Interspeech.2021-941},
arxivid = {2104.01466},
}

@article{4d052febd3625bc0fd09c81d51ec1c4d58bad95d,
title = {Multi-Frequency Information Enhanced Channel Attention Module for Speaker Representation Learning},
year = {2022},
url = {https://www.semanticscholar.org/paper/4d052febd3625bc0fd09c81d51ec1c4d58bad95d},
abstract = {Recently, attention mechanisms have been applied successfully in neural network-based speaker veriﬁcation systems. Incorporating the Squeeze-and-Excitation block into convolutional neural networks has achieved remarkable performance. How-ever, it uses global average pooling (GAP) to simply average the features along time and frequency dimensions, which is in-capable of preserving sufﬁcient speaker information in the feature maps. In this study, we show that GAP is a special case of a discrete cosine transform (DCT) on time-frequency domain mathematically using only the lowest frequency component in frequency decomposition. To strengthen the speaker information extraction ability, we propose to utilize multi-frequency information and design two novel and effective attention modules, called Single-Frequency Single-Channel (SFSC) attention module and Multi-Frequency Single-Channel (MFSC) attention module. The proposed attention modules can effectively capture more speaker information from multiple frequency components on the basis of DCT. We conduct comprehensive experi-ments on the VoxCeleb datasets and a probe evaluation on the 1 st 48-UTD forensic corpus. Experimental results demonstrate that our proposed SFSC and MFSC attention modules can ef-ﬁciently generate more discriminative speaker representations and outperform ResNet34-SE and ECAPA-TDNN systems with relative 20.9% and 20.2% reduction in EER, without adding extra network parameters.},
author = {Mufan Sang and J. Hansen},
journal = {ArXiv},
volume = {abs/2207.04540},
pages = {321-325},
doi = {10.48550/arXiv.2207.04540},
arxivid = {2207.04540},
}

@article{41f82e232a9b0e559b5c7ed5decb658d1f437928,
title = {Simple Attention Module Based Speaker Verification with Iterative Noisy Label Detection},
year = {2021},
url = {https://www.semanticscholar.org/paper/41f82e232a9b0e559b5c7ed5decb658d1f437928},
abstract = {Recently, the attention mechanism such as squeeze-and-excitation module (SE) and convolutional block attention module (CBAM) has achieved great success in deep learning-based speaker verification system. This paper introduces an alternative effective yet simple one, i.e., simple attention module (SimAM), for speaker verification. The SimAM module is a plug-and-play module without extra modal parameters. In addition, we propose a noisy label detection method to iteratively filter out the data samples with a noisy label from the training data, considering that a large-scale dataset labeled with human annotation or other automated processes may contain noisy labels. Data with the noisy label may over parameterize a deep neural network (DNN) and result in a performance drop due to the memorization effect of the DNN. Experiments are conducted on VoxCeleb dataset. The speaker verification model with SimAM achieves the 0.675% equal error rate (EER) on VoxCeleb1 original test trials. Our proposed iterative noisy label detection method further reduces the EER to 0.643%.},
author = {Xiaoyi Qin and N. Li and Chao Weng and Dan Su and Ming Li},
journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {6722-6726},
doi = {10.1109/icassp43922.2022.9746294},
arxivid = {2110.06534},
}

@article{ed9b09325bd810b78821b8cbd14cd1abdfc67119,
title = {The xx205 System for the VoxCeleb Speaker Recognition Challenge 2020},
year = {2020},
url = {https://www.semanticscholar.org/paper/ed9b09325bd810b78821b8cbd14cd1abdfc67119},
abstract = {S2 TL;DR: This report describes the systems submitted to the first and second tracks of the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020, which ranked second in both tracks, and investigates multiple CNN architectures including ResNet, Res2Net and dual path network to extract the x-vectors.},
author = {Xu Xiang},
journal = {ArXiv},
volume = {abs/2011.00200},
pages = {null},
arxivid = {2011.00200},
}

@article{9ea35ed1155c6938cf1f9181f89afd53b4af6110,
title = {Inclusive Speaker Verification with Adaptive thresholding},
year = {2021},
url = {https://www.semanticscholar.org/paper/9ea35ed1155c6938cf1f9181f89afd53b4af6110},
abstract = {While using a speaker verification (SV) based system in a commercial application, it is important that customers have an inclusive experience irrespective of their gender, age, or ethnicity. In this paper, we analyze the impact of gender and age on SV and find that for a desired common False Acceptance Rate (FAR) across different gender and age groups, the False Rejection Rate (FRR) is different for different gender and age groups. To optimize FRR for all users for a desired FAR, we propose a context (e.g. gender, age) adaptive thresholding framework for SV. The context can be available as prior information for many practical applications. We also propose a concatenated gender/age detection model to algorithmically derive the context in absence of such prior information. We experimentally show that our context-adaptive thresholding method is effective in building a more efficient inclusive SV system. Specifically, we show that we can reduce FRR for specific gender for a desired FAR on voxceleb1 test set by using gender-specific thresholds. Similar analysis on OGI kids’ speech corpus shows that by using age specific threshold, we can significantly reduce FRR for certain age groups for desired FAR.},
author = {Navdeep Jain and Hongcheng Wang},
journal = {ArXiv},
volume = {abs/2111.05501},
pages = {null},
arxivid = {2111.05501},
}

@article{ab5b5496bda3002c0072b5ef670b310029ad7b3d,
title = {Cross-Lingual Speaker Verification with Domain-Balanced Hard Prototype Mining and Language-Dependent Score Normalization},
year = {2020},
url = {https://www.semanticscholar.org/paper/ab5b5496bda3002c0072b5ef670b310029ad7b3d},
abstract = {In this paper we describe the top-scoring IDLab submission for the text-independent task of the Short-duration Speaker Verification (SdSV) Challenge 2020. The main difficulty of the challenge exists in the large degree of varying phonetic overlap between the potentially cross-lingual trials, along with the limited availability of in-domain DeepMine Farsi training data. We introduce domain-balanced hard prototype mining to fine-tune the state-of-the-art ECAPA-TDNN x-vector based speaker embedding extractor. The sample mining technique efficiently exploits speaker distances between the speaker prototypes of the popular AAM-softmax loss function to construct challenging training batches that are balanced on the domain-level. To enhance the scoring of cross-lingual trials, we propose a language-dependent s-norm score normalization. The imposter cohort only contains data from the Farsi target-domain which simulates the enrollment data always being Farsi. In case a Gaussian-Backend language model detects the test speaker embedding to contain English, a cross-language compensation offset determined on the AAM-softmax speaker prototypes is subtracted from the maximum expected imposter mean score. A fusion of five systems with minor topological tweaks resulted in a final MinDCF and EER of 0.065 and 1.45% respectively on the SdSVC evaluation set.},
author = {Jenthe Thienpondt and Brecht Desplanques and Kris Demuynck},
journal = {ArXiv},
volume = {abs/2007.07689},
pages = {null},
doi = {10.21437/interspeech.2020-2662},
arxivid = {2007.07689},
}

@article{389cd9824428be98a710f5f4de67121a70c15fd3,
title = {X-Vectors: Robust DNN Embeddings for Speaker Recognition},
year = {2018},
url = {https://www.semanticscholar.org/paper/389cd9824428be98a710f5f4de67121a70c15fd3},
abstract = {In this paper, we use data augmentation to improve performance of deep neural network (DNN) embeddings for speaker recognition. The DNN, which is trained to discriminate between speakers, maps variable-length utterances to fixed-dimensional embeddings that we call x-vectors. Prior studies have found that embeddings leverage large-scale training datasets better than i-vectors. However, it can be challenging to collect substantial quantities of labeled data for training. We use data augmentation, consisting of added noise and reverberation, as an inexpensive method to multiply the amount of training data and improve robustness. The x-vectors are compared with i-vector baselines on Speakers in the Wild and NIST SRE 2016 Cantonese. We find that while augmentation is beneficial in the PLDA classifier, it is not helpful in the i-vector extractor. However, the x-vector DNN effectively exploits data augmentation, due to its supervised training. As a result, the x-vectors achieve superior performance on the evaluation datasets.},
author = {David Snyder and D. Garcia-Romero and Gregory Sell and Daniel Povey and S. Khudanpur},
journal = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
volume = {null},
pages = {5329-5333},
doi = {10.1109/ICASSP.2018.8461375},
}

@article{2b314847feb05ecbae83bb7615f343bd1f299ec2,
title = {How to Aggregate Acoustic Delta Features for Deep Speaker Embeddings},
year = {2020},
url = {https://www.semanticscholar.org/paper/2b314847feb05ecbae83bb7615f343bd1f299ec2},
abstract = {Speaker verification based on deep speaker embeddings (DSE) network outperformed traditional i- vectors systems. Afterward, to improve the performance, various researches have been conducting and data augmentation methods are one of them. In this paper, we focus on acoustic delta features augmentation and their aggregation methods for DSE networks, X-vectors and MobileVoxNet. For CNN-based MobileVoxNet, we re-design the architecture to aggregate delta features in deeper layer with squeeze and excitation (SE) module. Experimental results show that the proposed methods achieve performance improvement compared to not using delta features on the VoxCeleb1 test dataset. We also compare the number of computations and parameters of models to analyze efficiency of the proposed methods.},
author = {Youngsam Kim and Jong-Hyuk Roh and Kwantae Cho and Sangrae Cho},
journal = {2020 International Conference on Information and Communication Technology Convergence (ICTC)},
volume = {null},
pages = {1225-1229},
doi = {10.1109/ICTC49870.2020.9289205},
}

@article{0e66dd4beea8ce1f76cb54d18902703bc44566dd,
title = {Tongji University Undergraduate Team for the VoxCeleb Speaker Recognition Challenge2020},
year = {2020},
url = {https://www.semanticscholar.org/paper/0e66dd4beea8ce1f76cb54d18902703bc44566dd},
abstract = {In this report, we discribe the submission of Tongji University undergraduate team to the CLOSE track of the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020 at Interspeech 2020. We applied the RSBU-CW module to the ResNet34 framework to improve the denoising ability of the network and better complete the speaker verification task in a complex environment.We trained two variants of ResNet,used score fusion and data-augmentation methods to improve the performance of the model. Our fusion of two selected systems for the CLOSE track achieves 0.2973 DCF and 4.9700\% EER on the challenge evaluation set.},
author = {Shufan Shen and Ran Miao and Yi Wang and Zhihua Wei},
journal = {ArXiv},
volume = {abs/2010.10145},
pages = {null},
arxivid = {2010.10145},
}

@article{18e2dbf47d3796f12c54f1247aa52b25d35f104c,
title = {The HCCL Speaker Verification System for Far-Field Speaker Verification Challenge},
year = {2021},
url = {https://www.semanticscholar.org/paper/18e2dbf47d3796f12c54f1247aa52b25d35f104c},
abstract = {This paper describes the systems submitted by team HCCL to the Far-Field Speaker Verification Challenge. Our previous work in the AIshell Speaker Verification Challenge 2019 shows that the powerful modeling abilities of Neural Network architectures can provide exceptional performence for this kind of task. Therefore, in this challenge, we focus on constructing deep Neural Network architectures based on TDNN, Resnet and Res2net blocks. Most of the developed systems consist of Neural Network embeddings are applied with PLDA backend. Firstly, the speed perturbation method is applied to augment data and significant performance improvements are achieved. Then, we explore the use of AMsoftmax loss function and propose to join a CE-loss branch when we train model using AMsoftmax loss. In addition, the impact of score normalization on perfomance is also investigated. The final system, a fusion of four systems, achieves minDCF 0.5342, EER 5.05% on task1 eval set, and achieves minDCF 0.5193, EER 5.47% on task3 eval set.},
author = {Zhuo Li and Ce Fang and Runqiu Xiao and Zhigao Chen and Wenchao Wang and Yonghong Yan},
journal = {ArXiv},
volume = {abs/2107.01329},
pages = {null},
arxivid = {2107.01329},
}

@article{a004f8f4c366763dec1d485d81fe6fad772d66b1,
title = {The SpeakIn System for VoxCeleb Speaker Recognition Challange 2021},
year = {2021},
url = {https://www.semanticscholar.org/paper/a004f8f4c366763dec1d485d81fe6fad772d66b1},
abstract = {This report describes our submission to the track 1 and track 2 of the VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC 2021). Both track 1 and track 2 share the same speaker verification system, which only uses VoxCeleb2-dev as our training set. This report explores several parts, including data augmentation, network structures, domain-based large margin fine-tuning, and back-end refinement. Our system is a fusion of 9 models and achieves first place in these two tracks of VoxSRC 2021. The minDCF of our submission is 0.1034, and the corresponding EER is 1.8460%.},
author = {Miao Zhao and Yufeng Ma and Min Liu and Minqiang Xu},
journal = {ArXiv},
volume = {abs/2109.01989},
pages = {null},
arxivid = {2109.01989},
}
